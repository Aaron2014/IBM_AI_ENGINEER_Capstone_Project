{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b65db62ed24a3fa9dfa6c3212aec63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/261482368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caac4d001fa54335a603e4b8c51b927c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.engine.functional.Functional at 0x1ede828ea90>,\n",
       " <keras.src.layers.core.dense.Dense at 0x1ede824c850>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.engine.input_layer.InputLayer at 0x1ede7f72940>,\n",
       " <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x1ede7f72f70>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede6847580>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede6847b50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede7fbb6d0>,\n",
       " <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x1ede801d820>,\n",
       " <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x1ede805d790>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8073490>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8073790>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede807f3a0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede807fbe0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede807f880>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80914c0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede805d3a0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8091e20>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede805dac0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8067f40>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede7fbbb80>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede809a4f0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8073c10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede805df40>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8080940>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8098520>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede80983a0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80a3640>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede80a3310>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede809ff70>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede80b3730>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80b33a0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede80b9c40>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede80c17c0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80b9fd0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede80ca2b0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede80d4820>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80ca6d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede80dc2e0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede80e28b0>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede80dc670>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80ea4f0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede80ea700>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede80f2c40>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80fe7c0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8102640>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede810bb20>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede810b3a0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede80f2790>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81135e0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede80f2370>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8119c10>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede8119af0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8122820>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8126ac0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81266a0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8126fa0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8122520>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8122040>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80b3e80>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8098c70>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede68460d0>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede8133fd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80d7430>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81066d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81062b0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80d7fd0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede80bfd00>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede809f2e0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80abdc0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede812dee0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede812d7c0>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede8138970>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede813dc70>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8138e50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede813de20>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede814f2e0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede814f940>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede814fac0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede815e2b0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede815e9d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede815ebe0>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede816c370>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede816c850>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede817c160>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8175cd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede81882b0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81889d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8188670>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede819b2e0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81664f0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede819be50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81668e0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede819be80>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede817ce50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8138ac0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8188490>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede816c4c0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80a99d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede814b5b0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81a4e50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede80ce700>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8181310>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8181430>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede81819d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede81a8b50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81b4730>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81bc2b0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede81b4fd0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81ae190>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81bc040>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede81c83a0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81b4820>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81d83a0>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede81d8430>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede81c32e0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81e0af0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81e0e20>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede81d4190>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81f1520>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81f2a00>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede81f2310>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81fc5b0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81ffa90>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede81ffb20>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede820c6d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8210970>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede820c460>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede820caf0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede821c070>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede821c250>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8219190>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81c8820>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81e5fd0>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede8210d30>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8223610>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede82230d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8223040>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8204220>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8204b50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8204bb0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede81b9310>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81b9c40>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81b9b50>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede822e100>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede822e2b0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8235790>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81b9f70>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede823ecd0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8238a30>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede822b940>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede81f5130>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede822bbe0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8252e80>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede822ba30>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede823e040>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede8238b80>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8267790>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede825b280>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede822ebe0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8252f10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8270790>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede82678e0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8285f10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8285d00>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8270fd0>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede8299fa0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8299eb0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede8285b80>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede8299160>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede822bfa0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede822e370>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede824bf40>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede81ae820>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x1ede81f5970>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x1ede81acd90>,\n",
       " <keras.src.layers.merging.add.Add at 0x1ede823e670>,\n",
       " <keras.src.layers.core.activation.Activation at 0x1ede8267670>,\n",
       " <keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x1ede828e430>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23591810 (90.00 MB)\n",
      "Trainable params: 4098 (16.01 KB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanji\\AppData\\Local\\Temp\\ipykernel_14828\\251737888.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  fit_history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "301/301 [==============================] - 2555s 8s/step - loss: 0.0225 - accuracy: 0.9942 - val_loss: 0.0071 - val_accuracy: 0.9980\n",
      "Epoch 2/2\n",
      "301/301 [==============================] - 2693s 9s/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.0044 - val_accuracy: 0.9989\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanji\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
